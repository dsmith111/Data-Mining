{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB;\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "from sklearn import preprocessing;\n",
    "from sklearn.manifold import MDS;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.cluster import KMeans;\n",
    "from sklearn.mixture import GaussianMixture;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from IPython.display import Image;\n",
    "import pydotplus;\n",
    "from copy import deepcopy;\n",
    "from sklearn.neighbors import KNeighborsClassifier;\n",
    "from sklearn.metrics import confusion_matrix;\n",
    "from sklearn.tree import DecisionTreeClassifier;\n",
    "from sklearn.tree._tree import TREE_LEAF;\n",
    "from sklearn import tree as treeDep;\n",
    "import seaborn as sn;\n",
    "from sklearn.metrics import plot_confusion_matrix;\n",
    "from DataProcessing import DataProcessing;\n",
    "from sklearn.linear_model import LinearRegression;\n",
    "\n",
    "\n",
    "# Problem 1, Linear Regression of USAFAO for total wheat production 2013-2019\n",
    "\n",
    "cereal_types = [\n",
    "    \"Wheat\", \"Rice\", \"Barley\", \"Maize\", \"Rye\", \"Oats\", \"Millet\", \"Sorghum\", \"Cereals, Other\"]\n",
    "\n",
    "cereal_codes = [\n",
    "    2511, 2805, 2513, 2514, 2515, 2516, 2517, 2518, 2520]\n",
    "\n",
    "\n",
    "# Import USAFAO\n",
    "USAFAO_data = pd.read_csv(\"USAFAO.csv\")\n",
    "\n",
    "# Clean up data\n",
    "cleaned_USAFAO = DataProcessing.cleanDF(cereal_codes, 'Item Code', USAFAO_data)\n",
    "\n",
    "print(len(USAFAO_data['Item']))\n",
    "print(len(cleaned_USAFAO['Item']))\n",
    "\n",
    "# Prepare data to for merging of feed and food\n",
    "drop_array = ['Area Abbreviation', 'Area','Area Code', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']\n",
    "grouped_USAFAO = cleaned_USAFAO.drop(drop_array, axis = 1)\n",
    "grouped_USAFAO = grouped_USAFAO.groupby(['Item','Item Code']).sum().reset_index()\n",
    "\n",
    "grouped_USAFAO = grouped_USAFAO.drop(['Item Code'], axis=1)\n",
    "grouped_USAFAO = grouped_USAFAO.rename(index=grouped_USAFAO['Item']).drop(\"Item\", axis=1)\n",
    "# print(grouped_USAFAO.head(2))\n",
    "x_train, x_test, y_train, y_test = train_test_split(grouped_USAFAO.drop(\"Y2013\", axis=1),grouped_USAFAO['Y2013'], random_state=42, test_size=.35 )\n",
    "\n",
    "reg = LinearRegression()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "reFormattedDF = pd.DataFrame(data={\"Year\":map(lambda num: num+1961, range(2014-1961))});\n",
    "for cereal in dict.fromkeys(cleaned_USAFAO['Item']).keys():\n",
    "    tempDF = deepcopy(grouped_USAFAO)\n",
    "    tempDF = tempDF.loc[cereal]\n",
    "    x_scaled = min_max_scaler.fit_transform(tempDF.values.reshape(-1, 1))\n",
    "    if(cereal == 'Wheat and products'):\n",
    "        saved_scale =  min_max_scaler.scale_\n",
    "    baseDF = pd.DataFrame(x_scaled, columns =[cereal])\n",
    "#     baseDF = pd.DataFrame(data={cereal:normalized})\n",
    "#     baseDF = pd.DataFrame(data={cereal:tempDF.values})\n",
    "    reFormattedDF = reFormattedDF.join(baseDF[cereal])\n",
    "\n",
    "print(reFormattedDF)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reFormattedDF['Year'].values.reshape(-1, 1),reFormattedDF.drop(\"Year\",axis=1), random_state=42, test_size=.22 )\n",
    "\n",
    "\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "errRateTrain = reg.score(x_train,y_train)\n",
    "errRateTest =  reg.score(x_test,y_test)\n",
    "\n",
    "print(\"Error on training {:.3f}\".format(errRateTrain))\n",
    "print(\"Error on testing {:.3f}\\n\".format(errRateTest))\n",
    "\n",
    "x = np.array(list(map(lambda num: num+2013, range(2020-2014))))\n",
    "y = reg.predict(np.array(list(map(lambda num: num+2013, range(2020-2014)))).reshape(-1, 1))\n",
    "print((x).shape)\n",
    "print((y).shape)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(reFormattedDF.columns)-1):\n",
    "    plt.scatter(reFormattedDF['Year'], reFormattedDF.drop('Year',axis=1).values[:,i], label=reFormattedDF.drop('Year',axis=1).columns[i])\n",
    "    plt.scatter(x, y[:,i], label=\"Predicted \" + reFormattedDF.drop('Year',axis=1).columns[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Scale is {}\\n\".format(saved_scale))\n",
    "print(\"2013 Predicted Amount:\")\n",
    "print(reg.predict(np.array([2013]).reshape(-1,1))[0][0]/saved_scale)\n",
    "print(\"\\n2013 Real Amount:\")\n",
    "print(reFormattedDF[\"Wheat and products\"].values[-1]/saved_scale)\n",
    "pred2018 = reg.predict(np.array([2019]).reshape(-1,1))[0][0]/saved_scale\n",
    "print(\"\\n2018 Predicted Amount:\")\n",
    "print(pred2018)\n",
    "\n",
    "print(\"\\nReal Bushels:\")\n",
    "print((1885156 *1000))\n",
    "\n",
    "print(\"\\nReal Kilograms:\")\n",
    "print((1885156 * 1000 * 27.2 ))\n",
    "\n",
    "print(\"\\nReal Metric Tonnes (100tonnes unit):\")\n",
    "print(((1885156 * 1000* 27.2) / 1000))\n",
    "    \n",
    "#--------------------------\n",
    "# Problem 2\n",
    "\n",
    "# Clean up data\n",
    "cleaned_FAO = DataProcessing.cleanDF(cereal_codes, 'Item Code', USAFAO_data)\n",
    "\n",
    "# Prepare data to for merging of feed and food\n",
    "drop_array = ['Area','Area Code','Area Abbreviation', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']\n",
    "grouped_FAO = cleaned_FAO.drop(drop_array, axis = 1)\n",
    "grouped_FAO = grouped_FAO.groupby(['Item','Item Code',]).sum().reset_index()\n",
    "\n",
    "# Create Yield over last 20 years column\n",
    "mean_USAFAO = DataProcessing.dropYears(grouped_FAO, 20, 2013)\n",
    "mean_USAFAO = DataProcessing.yieldAvg(mean_USAFAO)\n",
    "\n",
    "# print(mean_FAO.head(20))\n",
    "\n",
    "# Problem 3\n",
    "\n",
    "# Import FAO\n",
    "FAO_data = pd.read_csv(\"FAO.csv\",encoding='latin1')\n",
    "\n",
    "# Clean up data\n",
    "cleaned_FAO = DataProcessing.cleanDF(cereal_codes, 'Item Code', FAO_data)\n",
    "\n",
    "print(len(FAO_data['Item']))\n",
    "print(len(cleaned_FAO['Item']))\n",
    "\n",
    "# Prepare data to for merging of feed and food\n",
    "drop_array = ['Area Abbreviation', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']\n",
    "grouped_FAO = cleaned_FAO.drop(drop_array, axis = 1)\n",
    "grouped_FAO = grouped_FAO.groupby(['Area','Area Code','Item','Item Code',]).sum().reset_index()\n",
    "# print(grouped_FAO.head(20))\n",
    "\n",
    "# Create Yield over last 20 years column\n",
    "mean_FAO = DataProcessing.dropYears(grouped_FAO, 20, 2013)\n",
    "mean_FAO = DataProcessing.yieldAvg(mean_FAO)\n",
    "\n",
    "print(mean_FAO.head(20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Problem 4 Data Exploration\n",
    "\n",
    "#Set up data for parallel coordinates (Column of Countries, Columns of cerals, cells of YA)\n",
    "dict_cereals = dict.fromkeys(mean_FAO['Item'])\n",
    "\n",
    "parallel_FAO = mean_FAO[mean_FAO['Area']==\"Null\"]['Area'].to_frame()\n",
    "key_col = \"Area\"\n",
    "\n",
    "unique_areas = dict.fromkeys(mean_FAO['Area']).keys()\n",
    "count = 0\n",
    "\n",
    "for key in unique_areas:\n",
    "\n",
    "    parallel_FAO = DataProcessing.parallel(mean_FAO, parallel_FAO, key, dict_cereals, key_col)\n",
    "\n",
    "# Parallel Coordinates of Ceral vs average yield\n",
    "xTicks = list(dict_cereals.keys())\n",
    "xTicks.append(\"Area\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [len(cereal_types)*2, len(cereal_types)]\n",
    "pd.plotting.parallel_coordinates(parallel_FAO[xTicks], 'Area')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "print(parallel_FAO)\n",
    "plt.show()\n",
    "\n",
    "# Normalize Data\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "norm_FAO = deepcopy(parallel_FAO)\n",
    "\n",
    "for cereal in xTicks[0:len(xTicks)-1]:\n",
    "    x_scaled = min_max_scaler.fit_transform(parallel_FAO[cereal].values.reshape(-1, 1))\n",
    "    temp_df = pd.DataFrame(x_scaled, columns =[cereal])\n",
    "    norm_FAO.update(temp_df)\n",
    "\n",
    "print(\"Normalized Table \\n\")\n",
    "print(norm_FAO)\n",
    "pd.plotting.parallel_coordinates(norm_FAO[xTicks], 'Area')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()\n",
    "\n",
    "# Scree Plot\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "scaled_data = preprocessing.scale(focused_data.T.values)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_data = pca.transform(scaled_data)\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percent')\n",
    "plt.xlabel(\"PC\")\n",
    "plt.title(\"Scree\")\n",
    "plt.show()\n",
    "\n",
    "# Run PCA on the data\n",
    "\n",
    "unique_areas_reduced = dict.fromkeys(norm_FAO['Area']).keys()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "mds_array=[]\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "emb = PCA(3, random_state=42)\n",
    "fitted_x = emb.fit_transform(focused_data.values)\n",
    "mds_array.append(fitted_x)    \n",
    "ax.scatter(fitted_x[:,0],fitted_x[:,1],fitted_x[:,2], s=40)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "ax.set_xlabel(\"First Component\")\n",
    "ax.set_ylabel(\"Second Component\")\n",
    "ax.set_zlabel(\"Third Component\")\n",
    "plt.title(\"Principal Component Analysis\")\n",
    "\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run MDS on the data\n",
    "\n",
    "unique_areas_reduced = dict.fromkeys(norm_FAO['Area']).keys()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "mds_array=[]\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "emb = MDS(3, random_state=26)\n",
    "fitted_x = emb.fit_transform(focused_data.values)\n",
    "mds_array.append(fitted_x)    \n",
    "ax.scatter(fitted_x[:,0],fitted_x[:,1],fitted_x[:,2], s=40)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Multidimensional Scaling\")\n",
    "\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run GM on the data\n",
    "fig = plt.figure(0)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(mds_array)):\n",
    "    focused_data = mds_array[i]\n",
    "    grp = GaussianMixture(2, random_state=31)\n",
    "    grp.fit(focused_data)\n",
    "    yhat = grp.predict(focused_data)\n",
    "    clusters = np.unique(yhat)\n",
    "    j = 1\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        row_ix = np.where(yhat == cluster)\n",
    "        if(j == 1):\n",
    "            name = \"High Yield \"\n",
    "        else:\n",
    "            name = \"Normal Yield\"\n",
    "        ax.scatter(focused_data[row_ix,0],focused_data[row_ix,1],focused_data[row_ix,2], s=40, label = name + \"cluster: {}\".format(j))      \n",
    "        \n",
    "        j+=1\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Gaussian Mixture\")\n",
    "\n",
    "plt.show()    \n",
    "\n",
    "# Run KMeans on the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "large_frame = pd.DataFrame(columns = [\"First\",\"Second\",\"Third\", \"clusters\"])\n",
    "\n",
    "headers = list(dict_cereals.keys())\n",
    "headers.append(\"clusters\")\n",
    "PC1 = pd.DataFrame(columns = headers)\n",
    "PC2 = pd.DataFrame(columns = headers)\n",
    "PC3 = pd.DataFrame(columns = headers)\n",
    "\n",
    "for i in range(len(mds_array)):\n",
    "\n",
    "    focused_data = mds_array[i]\n",
    "    kmeans = KMeans(n_clusters=2, random_state=54)\n",
    "    yhat = kmeans.fit_predict(focused_data)\n",
    "    temp = focused_data\n",
    "    temp = pd.DataFrame(temp,columns=[\"First\",\"Second\",\"Third\"])\n",
    "    temp['clusters'] = yhat\n",
    "\n",
    "    df1 = temp[temp['clusters']==0]\n",
    "    df2 = temp[temp['clusters']==1]\n",
    "    \n",
    "    set1 = norm_FAO[temp['clusters']==0]\n",
    "    set2 = norm_FAO[temp['clusters']==1]\n",
    "#     set3 = norm_FAO[temp['clusters']==2]\n",
    "#     df4 = temp[temp['clusters']==3]\n",
    "#     df5 = temp[temp['clusters']==4]\n",
    "    \n",
    "    ax.scatter(df1['First'],df1['Second'],df1['Third'],c=\"blue\", s=40, label = 'High Yield Cluster: 1')\n",
    "    ax.scatter(df2['First'],df2['Second'],df2['Third'],c=\"red\", s=40, label = 'Normal Yield Cluster: 2')\n",
    "#     ax.scatter(df3['First'],df3['Second'],df3['Third'],c=\"green\", s=40, label = 'Cluster: 3')\n",
    "#     ax.scatter(df4['First'],df4['Second'],df4['Third'],c=\"orange\", s=40, label = 'Cluster: 4')\n",
    "#     ax.scatter(df5['First'],df5['Second'],df5['Third'],c=\"cyan\", s=40, label = 'Cluster: 5')\n",
    "    large_frame=large_frame.append(temp, ignore_index=True)\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run Parallel on the data\n",
    "fig = plt.figure()\n",
    "names = (\"High Yield\", \"Low Yield\")\n",
    "pd.plotting.parallel_coordinates(large_frame, 'clusters', color=('blue','red'), sort_labels=True)\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Parallel Coordinate Plot of Clusters\")\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Problem 5 Outlier Removal\n",
    "\n",
    "\n",
    "\n",
    "#Set up data for parallel coordinates (Column of Countries, Columns of cerals, cells of YA)\n",
    "dict_cereals = dict.fromkeys(mean_FAO['Item'])\n",
    "\n",
    "parallel_FAO = mean_FAO[mean_FAO['Area']==\"Null\"]['Area'].to_frame()\n",
    "key_col = \"Area\"\n",
    "\n",
    "unique_areas = dict.fromkeys(mean_FAO['Area']).keys()\n",
    "count = 0\n",
    "\n",
    "for key in unique_areas:\n",
    "\n",
    "    parallel_FAO = DataProcessing.parallel(mean_FAO, parallel_FAO, key, dict_cereals, key_col)\n",
    "\n",
    "\n",
    "# Normalize Data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "std_FAO = deepcopy(parallel_FAO)\n",
    "\n",
    "\n",
    "for cereal in xTicks[0:len(xTicks)-1]:\n",
    "    x_scaled = scaler.fit(std_FAO[cereal].values.reshape(-1, 1))\n",
    "    std_FAO = std_FAO[np.array(list(map(lambda elem: elem <= (1*x_scaled.scale_), std_FAO[cereal].values))).reshape(-1,1)]\n",
    "    \n",
    "\n",
    "print(\"Std Table \\n\")\n",
    "print(std_FAO)\n",
    "\n",
    "pd.plotting.parallel_coordinates(std_FAO[xTicks], 'Area')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()\n",
    "\n",
    "norm_FAO = deepcopy(std_FAO)\n",
    "\n",
    "for cereal in xTicks[0:len(xTicks)-1]:\n",
    "    x_scaled = min_max_scaler.fit_transform(norm_FAO[cereal].values.reshape(-1, 1))\n",
    "    temp_df = pd.DataFrame(x_scaled, columns =[cereal])\n",
    "    norm_FAO.update(temp_df)\n",
    "\n",
    "print(\"Normalized Table \\n\")\n",
    "print(norm_FAO)\n",
    "pd.plotting.parallel_coordinates(norm_FAO[xTicks], 'Area')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()\n",
    "\n",
    "# Scree Plot\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "scaled_data = preprocessing.scale(focused_data.T.values)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_data = pca.transform(scaled_data)\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percent')\n",
    "plt.xlabel(\"PC\")\n",
    "plt.title(\"Scree\")\n",
    "plt.show()\n",
    "\n",
    "# Run PCA on the data\n",
    "\n",
    "unique_areas_reduced = dict.fromkeys(norm_FAO['Area']).keys()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "mds_array=[]\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "emb = PCA(3, random_state=42)\n",
    "fitted_x = emb.fit_transform(focused_data.values)\n",
    "mds_array.append(fitted_x)    \n",
    "ax.scatter(fitted_x[:,0],fitted_x[:,1],fitted_x[:,2], s=40)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "ax.set_xlabel(\"First Component\")\n",
    "ax.set_ylabel(\"Second Component\")\n",
    "ax.set_zlabel(\"Third Component\")\n",
    "plt.title(\"Principal Component Analysis\")\n",
    "\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run MDS on the data\n",
    "\n",
    "unique_areas_reduced = dict.fromkeys(norm_FAO['Area']).keys()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "mds_array=[]\n",
    "focused_data = norm_FAO.drop('Area', axis=1)\n",
    "emb = MDS(3, random_state=26)\n",
    "fitted_x = emb.fit_transform(focused_data.values)\n",
    "mds_array.append(fitted_x)    \n",
    "ax.scatter(fitted_x[:,0],fitted_x[:,1],fitted_x[:,2], s=40)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Multidimensional Scaling\")\n",
    "\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run GM on the data\n",
    "fig = plt.figure(0)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(mds_array)):\n",
    "    focused_data = mds_array[i]\n",
    "    grp = GaussianMixture(2, random_state=31)\n",
    "    grp.fit(focused_data)\n",
    "    yhat = grp.predict(focused_data)\n",
    "    clusters = np.unique(yhat)\n",
    "    j = 1\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        row_ix = np.where(yhat == cluster)\n",
    "        if(j == 1):\n",
    "            name = \"Normal Yield \"\n",
    "        else:\n",
    "            name = \"Low Yield\"\n",
    "        ax.scatter(focused_data[row_ix,0],focused_data[row_ix,1],focused_data[row_ix,2], s=40, label = name + \"cluster: {}\".format(j))      \n",
    "        \n",
    "        j+=1\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Gaussian Mixture\")\n",
    "\n",
    "plt.show()    \n",
    "\n",
    "# Run KMeans on the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "large_frame = pd.DataFrame(columns = [\"First\",\"Second\",\"Third\", \"clusters\"])\n",
    "\n",
    "headers = list(dict_cereals.keys())\n",
    "headers.append(\"clusters\")\n",
    "PC1 = pd.DataFrame(columns = headers)\n",
    "PC2 = pd.DataFrame(columns = headers)\n",
    "PC3 = pd.DataFrame(columns = headers)\n",
    "\n",
    "for i in range(len(mds_array)):\n",
    "\n",
    "    focused_data = mds_array[i]\n",
    "    kmeans = KMeans(n_clusters=2, random_state=54)\n",
    "    yhat = kmeans.fit_predict(focused_data)\n",
    "    temp = focused_data\n",
    "    temp = pd.DataFrame(temp,columns=[\"First\",\"Second\",\"Third\"])\n",
    "    temp['clusters'] = yhat\n",
    "\n",
    "    df1 = temp[temp['clusters']==0]\n",
    "    df2 = temp[temp['clusters']==1]\n",
    "    \n",
    "#     set1 = norm_FAO[temp['clusters']==0]\n",
    "#     set2 = norm_FAO[temp['clusters']==1]\n",
    "#     set3 = norm_FAO[temp['clusters']==2]\n",
    "#     df4 = temp[temp['clusters']==3]\n",
    "#     df5 = temp[temp['clusters']==4]\n",
    "    \n",
    "    ax.scatter(df1['First'],df1['Second'],df1['Third'],c=\"blue\", s=40, label = 'Normal Yield Cluster: 1')\n",
    "    ax.scatter(df2['First'],df2['Second'],df2['Third'],c=\"red\", s=40, label = 'Low Yield Cluster: 2')\n",
    "#     ax.scatter(df3['First'],df3['Second'],df3['Third'],c=\"green\", s=40, label = 'Cluster: 3')\n",
    "#     ax.scatter(df4['First'],df4['Second'],df4['Third'],c=\"orange\", s=40, label = 'Cluster: 4')\n",
    "#     ax.scatter(df5['First'],df5['Second'],df5['Third'],c=\"cyan\", s=40, label = 'Cluster: 5')\n",
    "    large_frame=large_frame.append(temp, ignore_index=True)\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "# Run Parallel on the data\n",
    "fig = plt.figure()\n",
    "names = (\"High Yield\", \"Low Yield\")\n",
    "pd.plotting.parallel_coordinates(large_frame, 'clusters', color=('blue','red'), sort_labels=True)\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"Parallel Coordinate Plot of Clusters\")\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB;\n",
    "# Problem 6\n",
    "print(temp)\n",
    "x_train, x_test, y_train, y_test = train_test_split(temp.drop(\"clusters\", axis=1),temp['clusters'], random_state=42, test_size=.30 )\n",
    "\n",
    "# K-nearest Neighbor\n",
    "\n",
    "iterations = 8\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    kN = KNeighborsClassifier(n_neighbors=i+1)\n",
    "    kN.fit(x_train, y_train)\n",
    "    training_accuracy.append(kN.score(x_train,y_train))\n",
    "    testing_accuracy.append(kN.score(x_test,y_test))\n",
    "    \n",
    "    \n",
    "    if i ==0:\n",
    "        continue\n",
    "    if kN.score(x_test,y_test) > max(testing_accuracy[:i]):\n",
    "        print(\"KNearest Neighbor\\n{} Neighbors\".format(i+1))\n",
    "        print(\"Accuracy on training {:.3f}\".format(kN.score(x_train,y_train)))\n",
    "        print(\"Accuracy on testing {:.3f}\".format(kN.score(x_test,y_test)))\n",
    "    \n",
    "# print(\"KNearest Neighbor\\n{} Neighbors\".format(i+1))\n",
    "print(\"Accuracy on training {:.3f}\".format(max(testing_accuracy[:5])))\n",
    "print(\"Accuracy on testing {:.3f}\".format(max(testing_accuracy[:5])))\n",
    "plt.plot(np.add(range(iterations),1), np.multiply(training_accuracy,100), c='blue', label='Training Data')\n",
    "plt.plot(np.add(range(iterations),1), np.multiply(testing_accuracy,100), c='orange', label=\"Testing Data\")\n",
    "plt.xlabel(\"Amount of Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Versus Number of K\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Decision Trees\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training {:.3f}\".format(tree.score(x_train,y_train)))\n",
    "print(\"Accuracy on testing {:.3f}\".format(tree.score(x_test,y_test)))\n",
    "print(\"Number of Leaves {}\".format(tree.get_n_leaves()))\n",
    "print(\"Tree Depth {}\".format(tree.get_depth()))\n",
    "\n",
    "dot_data = treeDep.export_graphviz(tree, out_file=None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "# graph.write_png('./tree.png')\n",
    "display(Image(graph.create_png()))\n",
    "\n",
    "# Prune\n",
    "levels = 30\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "for i in range(levels):\n",
    "    \n",
    "    DataProcessing.prune_index(tree.tree_, 0, i+1)\n",
    "    training_accuracy.append(tree.score(x_train,y_train))\n",
    "    testing_accuracy.append(tree.score(x_test,y_test))\n",
    "    if i ==0:\n",
    "        continue\n",
    "    if tree.score(x_test,y_test) > max(testing_accuracy[:i]) or i == levels-1:\n",
    "        print(\"Prune Level: {}\".format(i+1))\n",
    "        print(\"Accuracy on training {:.3f}\".format(tree.score(x_train,y_train)))\n",
    "        print(\"Accuracy on testing {:.3f}\".format(tree.score(x_test,y_test)))\n",
    "        print(\"Number of Leaves {}\".format(tree.get_n_leaves()))\n",
    "        print(\"Tree Depth {}\".format(tree.get_depth()))\n",
    "\n",
    "        dot_data = treeDep.export_graphviz(tree, out_file=None)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "        # graph.write_png('./treepruned.png')\n",
    "        display(Image(graph.create_png()))\n",
    "\n",
    "plt.plot(np.add(range(levels),1), np.multiply(training_accuracy,100), c='blue', label='Training Data')\n",
    "plt.plot(np.add(range(levels),1), np.multiply(testing_accuracy,100), c='orange', label=\"Testing Data\")\n",
    "plt.xlabel(\"Prune Level\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Tree Accuracy Versus Pruning\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run Naive Bayes with different distributions\n",
    "\n",
    "# Run MDS on the data\n",
    "\n",
    "unique_areas_reduced = dict.fromkeys(norm_FAO['Area']).keys()\n",
    "\n",
    "states = 50\n",
    "mds_array=[]\n",
    "\n",
    "for i in range(states):\n",
    "    focused_data = norm_FAO.drop('Area', axis=1)\n",
    "    emb = MDS(3, random_state=i)\n",
    "    fitted_x = emb.fit_transform(focused_data.values)\n",
    "    mds_array.append(fitted_x)    \n",
    "\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "\n",
    "# Run KMeans on the data\n",
    "large_frame = pd.DataFrame(columns = [\"First\",\"Second\",\"Third\", \"clusters\"])\n",
    "\n",
    "headers = list(dict_cereals.keys())\n",
    "headers.append(\"clusters\")\n",
    "PC1 = pd.DataFrame(columns = headers)\n",
    "PC2 = pd.DataFrame(columns = headers)\n",
    "PC3 = pd.DataFrame(columns = headers)\n",
    "\n",
    "for i in range(len(mds_array)):\n",
    "\n",
    "    focused_data = mds_array[i]\n",
    "    kmeans = KMeans(n_clusters=2, random_state=54)\n",
    "    yhat = kmeans.fit_predict(focused_data)\n",
    "    temp = focused_data\n",
    "    temp = pd.DataFrame(temp,columns=[\"First\",\"Second\",\"Third\"])\n",
    "    temp['clusters'] = yhat\n",
    "\n",
    "    df1 = temp[temp['clusters']==0]\n",
    "    df2 = temp[temp['clusters']==1]\n",
    "    \n",
    "    large_frame=large_frame.append(temp, ignore_index=True)\n",
    "    \n",
    "    # Naive Bayes\n",
    "    x_train, x_test, y_train, y_test = train_test_split(temp.drop(\"clusters\", axis=1),temp['clusters'], random_state=42, test_size=.30 )\n",
    "    mdl = GaussianNB()\n",
    "    mdl.fit(x_train, y_train)\n",
    "    training_accuracy.append(mdl.score(x_train,y_train))\n",
    "    testing_accuracy.append(mdl.score(x_test,y_test))\n",
    "    \n",
    "    if i ==0:\n",
    "        continue\n",
    "    if mdl.score(x_test,y_test) > max(testing_accuracy[:i]):\n",
    "        print(\"Naive Bayes\\n Distribution {}\".format(i+1))\n",
    "        print(\"Accuracy on training {:.3f}\".format(mdl.score(x_train,y_train)))\n",
    "        print(\"Accuracy on testing {:.3f}\".format(mdl.score(x_test,y_test)))\n",
    "        savedx1 = df1\n",
    "        savedx2 = df2\n",
    "        \n",
    "#     errRateNBTrain = 1 - mdl.score(x_train,y_train)\n",
    "#     errRateNBTest = 1 - mdl.score(x_test,y_test)\n",
    "    \n",
    "    \n",
    "#     print(\"Error on training {:.3f}\".format(errRateNBTrain))\n",
    "#     print(\"Error on testing {:.3f}\\n\".format(errRateNBTest))\n",
    "\n",
    "plt.plot(np.add(range(len(mds_array)),1), np.multiply(training_accuracy,100), c='blue', label='Training Data')\n",
    "plt.plot(np.add(range(len(mds_array)),1), np.multiply(testing_accuracy,100), c='orange', label=\"Testing Data\")\n",
    "plt.xlabel(\"Distrubtion by Random State\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Versus Distribution State\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(savedx1['First'],savedx1['Second'],savedx1['Third'],c=\"blue\", s=40, label = 'Normal Yield')\n",
    "ax.scatter(savedx2['First'],savedx2['Second'],savedx2['Third'],c=\"red\", s=40, label = 'Low Yield')\n",
    "plt.legend()\n",
    "ax.set_xlabel(\"First Dimension\")\n",
    "ax.set_ylabel(\"Second Dimension\")\n",
    "ax.set_zlabel(\"Third Dimension\")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
